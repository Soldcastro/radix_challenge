{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/15 01:17:22 WARN Utils: Your hostname, DESKTOP-TME356J resolves to a loopback address: 127.0.1.1; using 172.29.31.176 instead (on interface eth0)\n",
      "23/09/15 01:17:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/15 01:17:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/15 01:17:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"airflow_app\") \\\n",
    "    .config('spark.executor.memory', '6g') \\\n",
    "    .config('spark.driver.memory', '6g') \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1048MB\") \\\n",
    "    .config(\"spark.port.maxRetries\", \"100\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = 'datalake/trusted'\n",
    "write_path = 'datalake/refined'\n",
    "days = 7\n",
    "days_in_sec = lambda i: i * 86400\n",
    "partition = 1\n",
    "partition_fields = ['ano']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/15 01:31:05 INFO InMemoryFileIndex: It took 42 ms to list leaf files for 1 paths.\n",
      "23/09/15 01:31:05 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "23/09/15 01:31:05 INFO DAGScheduler: Got job 4 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/15 01:31:05 INFO DAGScheduler: Final stage: ResultStage 4 (load at NativeMethodAccessorImpl.java:0)\n",
      "23/09/15 01:31:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/15 01:31:05 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/15 01:31:05 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/15 01:31:05 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 103.5 KiB, free 3.4 GiB)\n",
      "23/09/15 01:31:05 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 37.3 KiB, free 3.4 GiB)\n",
      "23/09/15 01:31:06 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.29.31.176:34579 (size: 37.3 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:31:06 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/15 01:31:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/15 01:31:06 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "23/09/15 01:31:06 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (172.29.31.176, executor driver, partition 0, PROCESS_LOCAL, 7608 bytes) \n",
      "23/09/15 01:31:06 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "23/09/15 01:31:06 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.29.31.176:34579 in memory (size: 35.4 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:31:06 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2190 bytes result sent to driver\n",
      "23/09/15 01:31:06 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 65 ms on 172.29.31.176 (executor driver) (1/1)\n",
      "23/09/15 01:31:06 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "23/09/15 01:31:06 INFO DAGScheduler: ResultStage 4 (load at NativeMethodAccessorImpl.java:0) finished in 0.624 s\n",
      "23/09/15 01:31:06 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/15 01:31:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "23/09/15 01:31:06 INFO DAGScheduler: Job 4 finished: load at NativeMethodAccessorImpl.java:0, took 0.673821 s\n",
      "23/09/15 01:31:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.29.31.176:34579 in memory (size: 35.4 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:31:06 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.29.31.176:34579 in memory (size: 37.3 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:31:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.29.31.176:34579 in memory (size: 7.7 KiB, free: 3.4 GiB)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trusted_df = spark.read.format(\"parquet\") \\\n",
    "        .load(source_path)\n",
    "except Exception:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pais: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- quantidade_confirmados: long (nullable = true)\n",
      " |-- quantidade_mortes: long (nullable = true)\n",
      " |-- quantidade_recuperados: long (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trusted_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/15 01:22:32 INFO DataSourceStrategy: Pruning directories with: \n",
      "23/09/15 01:22:32 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/15 01:22:32 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/15 01:22:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 202.4 KiB, free 3.4 GiB)\n",
      "23/09/15 01:22:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 3.4 GiB)\n",
      "23/09/15 01:22:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.29.31.176:34579 (size: 35.4 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:22:32 INFO SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/15 01:22:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 18124764 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/15 01:22:32 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/15 01:22:32 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/15 01:22:32 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/15 01:22:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/15 01:22:32 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/15 01:22:32 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/15 01:22:32 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 20.7 KiB, free 3.4 GiB)\n",
      "23/09/15 01:22:32 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 3.4 GiB)\n",
      "23/09/15 01:22:32 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.29.31.176:34579 (size: 7.7 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:22:32 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/15 01:22:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/15 01:22:32 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "23/09/15 01:22:32 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.29.31.176, executor driver, partition 0, PROCESS_LOCAL, 8989 bytes) \n",
      "23/09/15 01:22:32 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "23/09/15 01:22:32 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=1/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-92570, partition values: [2021,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+\n",
      "|       pais|estado|latitude|longitude|               data|quantidade_confirmados|quantidade_mortes|quantidade_recuperados| ano|mes|\n",
      "+-----------+------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-01 00:00:00|                  null|             null|                 41727|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-02 00:00:00|                  null|             null|                 41727|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-03 00:00:00|                  null|             null|                 41727|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-04 00:00:00|                  null|             null|                 42530|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-05 00:00:00|                  null|             null|                 42666|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-06 00:00:00|                  null|             null|                 42666|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-07 00:00:00|                  null|             null|                 43291|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-08 00:00:00|                  null|             null|                 43440|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-09 00:00:00|                  null|             null|                 43740|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-10 00:00:00|                  null|             null|                 43948|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-11 00:00:00|                  null|             null|                 44137|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-12 00:00:00|                  null|             null|                 44608|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-13 00:00:00|                  null|             null|                 44850|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-14 00:00:00|                  null|             null|                 45298|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-15 00:00:00|                  null|             null|                 45434|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-16 00:00:00|                  null|             null|                 45465|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-17 00:00:00|                  null|             null|                 45868|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-18 00:00:00|                  null|             null|                 46359|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-19 00:00:00|                  null|             null|                 46554|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-20 00:00:00|                  null|             null|                 46759|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-21 00:00:00|                  null|             null|                 46887|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-22 00:00:00|                  null|             null|                 46912|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-23 00:00:00|                  null|             null|                 46943|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-24 00:00:00|                  null|             null|                 47298|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-25 00:00:00|                  null|             null|                 47365|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-26 00:00:00|                  null|             null|                 47459|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-27 00:00:00|                  null|             null|                 47549|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-28 00:00:00|                  null|             null|                 47583|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-29 00:00:00|                  null|             null|                 47606|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-30 00:00:00|                  null|             null|                 47609|2021|  1|\n",
      "|Afghanistan|  null|33.93911|67.709953|2021-01-31 00:00:00|                  null|             null|                 47679|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-01 00:00:00|                     0|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-02 00:00:00|                   675|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-03 00:00:00|                   447|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-04 00:00:00|                   185|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-05 00:00:00|                   660|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-06 00:00:00|                   725|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-07 00:00:00|                   697|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-08 00:00:00|                   673|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-09 00:00:00|                   655|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-10 00:00:00|                   562|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-11 00:00:00|                   376|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-12 00:00:00|                   656|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-13 00:00:00|                   707|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-14 00:00:00|                   660|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-15 00:00:00|                   641|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-16 00:00:00|                   581|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-17 00:00:00|                   474|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-18 00:00:00|                   292|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-19 00:00:00|                   586|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-20 00:00:00|                   670|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-21 00:00:00|                   678|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-22 00:00:00|                   739|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-23 00:00:00|                   786|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-24 00:00:00|                   833|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-25 00:00:00|                   538|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-26 00:00:00|                   879|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-27 00:00:00|                   876|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-28 00:00:00|                   887|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-29 00:00:00|                   896|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-30 00:00:00|                   901|             null|                  null|2021|  1|\n",
      "|    Albania|  null| 41.1533|  20.1683|2021-01-31 00:00:00|                   876|             null|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-01 00:00:00|                  null|                6|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-02 00:00:00|                  null|                7|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-03 00:00:00|                  null|                3|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-04 00:00:00|                  null|                5|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-05 00:00:00|                  null|                5|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-06 00:00:00|                  null|                4|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-07 00:00:00|                  null|                6|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-08 00:00:00|                  null|                6|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-09 00:00:00|                  null|                5|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-10 00:00:00|                  null|                4|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-11 00:00:00|                  null|                5|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-12 00:00:00|                  null|                4|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-13 00:00:00|                  null|                3|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-14 00:00:00|                  null|                3|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-15 00:00:00|                  null|                5|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-16 00:00:00|                  null|                4|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-17 00:00:00|                  null|                5|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-18 00:00:00|                  null|                4|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-19 00:00:00|                  null|                3|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-20 00:00:00|                  null|                6|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-21 00:00:00|                  null|                4|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-22 00:00:00|                  null|                3|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-23 00:00:00|                  null|                5|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-24 00:00:00|                  null|                2|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-25 00:00:00|                  null|                3|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-26 00:00:00|                  null|                5|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-27 00:00:00|                  null|                6|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-28 00:00:00|                  null|                4|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-29 00:00:00|                  null|                3|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-30 00:00:00|                  null|                4|                  null|2021|  1|\n",
      "|    Algeria|  null| 28.0339|   1.6596|2021-01-31 00:00:00|                  null|                3|                  null|2021|  1|\n",
      "|    Andorra|  null| 42.5063|   1.5218|2021-01-01 00:00:00|                    68|             null|                  null|2021|  1|\n",
      "|    Andorra|  null| 42.5063|   1.5218|2021-01-02 00:00:00|                    49|             null|                  null|2021|  1|\n",
      "|    Andorra|  null| 42.5063|   1.5218|2021-01-03 00:00:00|                    26|             null|                  null|2021|  1|\n",
      "|    Andorra|  null| 42.5063|   1.5218|2021-01-04 00:00:00|                    57|             null|                  null|2021|  1|\n",
      "|    Andorra|  null| 42.5063|   1.5218|2021-01-05 00:00:00|                    59|             null|                  null|2021|  1|\n",
      "|    Andorra|  null| 42.5063|   1.5218|2021-01-06 00:00:00|                    40|             null|                  null|2021|  1|\n",
      "|    Andorra|  null| 42.5063|   1.5218|2021-01-07 00:00:00|                     0|             null|                  null|2021|  1|\n",
      "+-----------+------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/15 01:22:32 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3031 bytes result sent to driver\n",
      "23/09/15 01:22:32 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 161 ms on 172.29.31.176 (executor driver) (1/1)\n",
      "23/09/15 01:22:32 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "23/09/15 01:22:32 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.213 s\n",
      "23/09/15 01:22:32 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/15 01:22:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "23/09/15 01:22:32 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.219322 s\n"
     ]
    }
   ],
   "source": [
    "trusted_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = (Window.partitionBy(\"pais\").orderBy(F.col(\"data\").cast('long')).rangeBetween(-days_in_sec(6), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trusted_df = trusted_df.withColumn(\"media_movel_confirmados\", F.avg(\"quantidade_confirmados\").over(window_spec))\n",
    "trusted_df = trusted_df.withColumn(\"media_movel_mortes\", F.avg(\"quantidade_mortes\").over(window_spec))\n",
    "trusted_df = trusted_df.withColumn(\"media_movel_recuperados\", F.avg(\"quantidade_recuperados\").over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pais: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- quantidade_confirmados: long (nullable = true)\n",
      " |-- quantidade_mortes: long (nullable = true)\n",
      " |-- quantidade_recuperados: long (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- media_movel_confirmados: double (nullable = true)\n",
      " |-- media_movel_mortes: double (nullable = true)\n",
      " |-- media_movel_recuperados: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trusted_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/15 01:34:39 INFO DataSourceStrategy: Pruning directories with: \n",
      "23/09/15 01:34:39 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/15 01:34:39 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/15 01:34:40 INFO CodeGenerator: Code generated in 115.739191 ms\n",
      "23/09/15 01:34:40 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 202.4 KiB, free 3.4 GiB)\n",
      "23/09/15 01:34:40 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 3.4 GiB)\n",
      "23/09/15 01:34:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.29.31.176:34579 (size: 35.4 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:34:40 INFO SparkContext: Created broadcast 7 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/15 01:34:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 18124764 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/15 01:34:40 INFO DAGScheduler: Registering RDD 17 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "23/09/15 01:34:40 INFO DAGScheduler: Got map stage job 5 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "23/09/15 01:34:40 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/15 01:34:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/15 01:34:40 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/15 01:34:40 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[17] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/15 01:34:40 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 22.5 KiB, free 3.4 GiB)\n",
      "23/09/15 01:34:40 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 3.4 GiB)\n",
      "23/09/15 01:34:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.29.31.176:34579 (size: 8.8 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:34:40 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/15 01:34:40 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[17] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "23/09/15 01:34:40 INFO TaskSchedulerImpl: Adding task set 5.0 with 4 tasks resource profile 0\n",
      "23/09/15 01:34:40 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (172.29.31.176, executor driver, partition 0, PROCESS_LOCAL, 8983 bytes) \n",
      "23/09/15 01:34:40 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6) (172.29.31.176, executor driver, partition 1, PROCESS_LOCAL, 8982 bytes) \n",
      "23/09/15 01:34:40 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 7) (172.29.31.176, executor driver, partition 2, PROCESS_LOCAL, 8971 bytes) \n",
      "23/09/15 01:34:40 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 8) (172.29.31.176, executor driver, partition 3, PROCESS_LOCAL, 8276 bytes) \n",
      "23/09/15 01:34:40 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
      "23/09/15 01:34:40 INFO Executor: Running task 2.0 in stage 5.0 (TID 7)\n",
      "23/09/15 01:34:40 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)\n",
      "23/09/15 01:34:40 INFO Executor: Running task 3.0 in stage 5.0 (TID 8)\n",
      "23/09/15 01:34:40 INFO CodeGenerator: Code generated in 45.133063 ms\n",
      "23/09/15 01:34:40 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=2/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-29961, partition values: [2020,2]\n",
      "23/09/15 01:34:40 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=6/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-70806, partition values: [2020,6]\n",
      "23/09/15 01:34:40 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=1/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-92570, partition values: [2021,1]\n",
      "23/09/15 01:34:40 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=10/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-85021, partition values: [2020,10]\n",
      "23/09/15 01:34:40 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "23/09/15 01:34:40 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "23/09/15 01:34:40 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=3/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-91024, partition values: [2021,3]\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=5/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-69194, partition values: [2020,5]\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=2/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-82866, partition values: [2021,2]\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=1/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-18944, partition values: [2020,1]\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=12/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-90446, partition values: [2020,12]\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=4/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-63397, partition values: [2020,4]\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=4/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-89024, partition values: [2021,4]\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=8/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-80567, partition values: [2020,8]\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=5/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-45684, partition values: [2021,5]\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=11/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-85908, partition values: [2020,11]\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=3/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-44720, partition values: [2020,3]\n",
      "23/09/15 01:34:41 INFO Executor: Finished task 3.0 in stage 5.0 (TID 8). 2284 bytes result sent to driver\n",
      "23/09/15 01:34:41 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 8) in 1489 ms on 172.29.31.176 (executor driver) (1/4)\n",
      "23/09/15 01:34:41 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=9/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-79051, partition values: [2020,9]\n",
      "23/09/15 01:34:42 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=7/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-76705, partition values: [2020,7]\n",
      "23/09/15 01:34:42 INFO Executor: Finished task 2.0 in stage 5.0 (TID 7). 2241 bytes result sent to driver\n",
      "23/09/15 01:34:42 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 7) in 1689 ms on 172.29.31.176 (executor driver) (2/4)\n",
      "23/09/15 01:34:42 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2284 bytes result sent to driver\n",
      "23/09/15 01:34:42 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1721 ms on 172.29.31.176 (executor driver) (3/4)\n",
      "23/09/15 01:34:42 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 2241 bytes result sent to driver\n",
      "23/09/15 01:34:42 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 1766 ms on 172.29.31.176 (executor driver) (4/4)\n",
      "23/09/15 01:34:42 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "23/09/15 01:34:42 INFO DAGScheduler: ShuffleMapStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 1.948 s\n",
      "23/09/15 01:34:42 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/09/15 01:34:42 INFO DAGScheduler: running: Set()\n",
      "23/09/15 01:34:42 INFO DAGScheduler: waiting: Set()\n",
      "23/09/15 01:34:42 INFO DAGScheduler: failed: Set()\n",
      "23/09/15 01:34:42 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.29.31.176:34579 in memory (size: 37.3 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:34:42 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1821118, minimum partition size: 1048576\n",
      "23/09/15 01:34:42 INFO CodeGenerator: Code generated in 53.91414 ms\n",
      "23/09/15 01:34:42 INFO CodeGenerator: Code generated in 97.406163 ms\n",
      "23/09/15 01:34:42 INFO CodeGenerator: Code generated in 68.061344 ms\n",
      "23/09/15 01:34:42 INFO CodeGenerator: Code generated in 33.008008 ms\n",
      "23/09/15 01:34:43 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/15 01:34:43 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/15 01:34:43 INFO DAGScheduler: Final stage: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/15 01:34:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "23/09/15 01:34:43 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/15 01:34:43 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[26] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/15 01:34:43 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 61.0 KiB, free 3.4 GiB)\n",
      "23/09/15 01:34:43 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 3.4 GiB)\n",
      "23/09/15 01:34:43 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.29.31.176:34579 (size: 22.2 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:34:43 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/15 01:34:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[26] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/15 01:34:43 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "23/09/15 01:34:43 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9) (172.29.31.176, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/09/15 01:34:43 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)\n",
      "23/09/15 01:34:43 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.29.31.176:34579 in memory (size: 8.8 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:34:43 INFO ShuffleBlockFetcherIterator: Getting 4 (1625.1 KiB) non-empty blocks including 4 (1625.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/15 01:34:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 48 ms\n",
      "23/09/15 01:34:43 INFO CodeGenerator: Code generated in 56.87969 ms (0 + 1) / 1]\n",
      "23/09/15 01:34:43 INFO CodeGenerator: Code generated in 15.341803 ms\n",
      "23/09/15 01:34:43 INFO CodeGenerator: Code generated in 36.734394 ms\n",
      "23/09/15 01:34:44 INFO CodeGenerator: Code generated in 45.355565 ms\n",
      "23/09/15 01:34:44 INFO CodeGenerator: Code generated in 17.530918 ms\n",
      "23/09/15 01:34:44 INFO CodeGenerator: Code generated in 13.662345 ms\n",
      "23/09/15 01:34:44 INFO CodeGenerator: Code generated in 11.542824 ms\n",
      "23/09/15 01:34:44 INFO CodeGenerator: Code generated in 21.077318 ms\n",
      "23/09/15 01:34:44 INFO CodeGenerator: Code generated in 8.024022 ms\n",
      "23/09/15 01:34:44 INFO CodeGenerator: Code generated in 29.588597 ms\n",
      "23/09/15 01:34:44 INFO CodeGenerator: Code generated in 14.063311 ms\n",
      "23/09/15 01:34:44 INFO CodeGenerator: Code generated in 13.447563 ms\n",
      "23/09/15 01:34:44 INFO CodeGenerator: Code generated in 66.178103 ms\n",
      "23/09/15 01:34:45 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:34:46 INFO CodeGenerator: Code generated in 47.442987 ms\n",
      "23/09/15 01:34:46 INFO CodeGenerator: Code generated in 26.449963 ms\n",
      "23/09/15 01:34:46 INFO CodeGenerator: Code generated in 34.355595 ms\n",
      "23/09/15 01:34:46 INFO CodeGenerator: Code generated in 37.71921 ms\n",
      "23/09/15 01:34:46 INFO CodeGenerator: Code generated in 32.495052 ms\n",
      "23/09/15 01:34:46 INFO CodeGenerator: Code generated in 43.04016 ms\n",
      "23/09/15 01:34:47 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:34:48 INFO CodeGenerator: Code generated in 17.278339 ms\n",
      "23/09/15 01:34:48 INFO CodeGenerator: Code generated in 17.927184 ms\n",
      "23/09/15 01:34:48 INFO CodeGenerator: Code generated in 18.20326 ms\n",
      "23/09/15 01:34:48 INFO CodeGenerator: Code generated in 13.687143 ms\n",
      "23/09/15 01:34:48 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 5884 bytes result sent to driver\n",
      "23/09/15 01:34:48 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 4825 ms on 172.29.31.176 (executor driver) (1/1)\n",
      "23/09/15 01:34:48 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "23/09/15 01:34:48 INFO DAGScheduler: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 4.906 s\n",
      "23/09/15 01:34:48 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/15 01:34:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "23/09/15 01:34:48 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 4.968822 s\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+-----------------------+------------------+-----------------------+\n",
      "|       pais|estado|latitude|longitude|               data|quantidade_confirmados|quantidade_mortes|quantidade_recuperados| ano|mes|media_movel_confirmados|media_movel_mortes|media_movel_recuperados|\n",
      "+-----------+------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+-----------------------+------------------+-----------------------+\n",
      "|Afghanistan|  null|33.93911|67.709953|2020-01-22 00:00:00|                  null|             null|                     0|2020|  1|                    0.0|               0.0|                    0.0|\n",
      "|Afghanistan|  null|33.93911|67.709953|2020-01-22 00:00:00|                  null|                0|                  null|2020|  1|                    0.0|               0.0|                    0.0|\n",
      "|Afghanistan|  null|33.93911|67.709953|2020-01-22 00:00:00|                     0|             null|                  null|2020|  1|                    0.0|               0.0|                    0.0|\n",
      "|Afghanistan|  null|33.93911|67.709953|2020-01-23 00:00:00|                  null|             null|                     0|2020|  1|                    0.0|               0.0|                    0.0|\n",
      "|Afghanistan|  null|33.93911|67.709953|2020-01-23 00:00:00|                  null|                0|                  null|2020|  1|                    0.0|               0.0|                    0.0|\n",
      "|Afghanistan|  null|33.93911|67.709953|2020-01-23 00:00:00|                     0|             null|                  null|2020|  1|                    0.0|               0.0|                    0.0|\n",
      "|Afghanistan|  null|33.93911|67.709953|2020-01-24 00:00:00|                  null|             null|                     0|2020|  1|                    0.0|               0.0|                    0.0|\n",
      "|Afghanistan|  null|33.93911|67.709953|2020-01-24 00:00:00|                  null|                0|                  null|2020|  1|                    0.0|               0.0|                    0.0|\n",
      "|Afghanistan|  null|33.93911|67.709953|2020-01-24 00:00:00|                     0|             null|                  null|2020|  1|                    0.0|               0.0|                    0.0|\n",
      "|Afghanistan|  null|33.93911|67.709953|2020-01-25 00:00:00|                  null|             null|                     0|2020|  1|                    0.0|               0.0|                    0.0|\n",
      "+-----------+------+--------+---------+-------------------+----------------------+-----------------+----------------------+----+---+-----------------------+------------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/15 01:34:48 INFO CodeGenerator: Code generated in 153.431724 ms\n"
     ]
    }
   ],
   "source": [
    "trusted_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df = trusted_df.select(\"pais\",\n",
    "                               \"data\",\n",
    "                               \"media_movel_confirmados\",\n",
    "                               \"media_movel_mortes\",\n",
    "                               \"media_movel_recuperados\",\n",
    "                               \"ano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df = refined_df.withColumn(\"media_movel_confirmados\", F.col(\"media_movel_confirmados\").cast(\"long\"))\n",
    "refined_df = refined_df.withColumn(\"media_movel_mortes\", F.col(\"media_movel_mortes\").cast(\"long\"))\n",
    "refined_df = refined_df.withColumn(\"media_movel_recuperados\", F.col(\"media_movel_recuperados\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pais: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- media_movel_confirmados: long (nullable = true)\n",
      " |-- media_movel_mortes: long (nullable = true)\n",
      " |-- media_movel_recuperados: long (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/15 01:42:52 INFO DataSourceStrategy: Pruning directories with: \n",
      "23/09/15 01:42:52 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/15 01:42:52 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/15 01:42:52 INFO CodeGenerator: Code generated in 32.12899 ms\n",
      "23/09/15 01:42:52 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 202.0 KiB, free 3.4 GiB)\n",
      "23/09/15 01:42:53 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 3.4 GiB)\n",
      "23/09/15 01:42:53 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.29.31.176:34579 (size: 35.2 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:42:53 INFO SparkContext: Created broadcast 10 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/15 01:42:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 18124764 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/15 01:42:53 INFO DAGScheduler: Registering RDD 30 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "23/09/15 01:42:53 INFO DAGScheduler: Got map stage job 7 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "23/09/15 01:42:53 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/15 01:42:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/15 01:42:53 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/15 01:42:53 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[30] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/15 01:42:53 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 19.9 KiB, free 3.4 GiB)\n",
      "23/09/15 01:42:53 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 3.4 GiB)\n",
      "23/09/15 01:42:53 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.29.31.176:34579 (size: 8.4 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:42:53 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/15 01:42:53 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[30] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "23/09/15 01:42:53 INFO TaskSchedulerImpl: Adding task set 8.0 with 4 tasks resource profile 0\n",
      "23/09/15 01:42:53 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10) (172.29.31.176, executor driver, partition 0, PROCESS_LOCAL, 8983 bytes) \n",
      "23/09/15 01:42:53 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 11) (172.29.31.176, executor driver, partition 1, PROCESS_LOCAL, 8977 bytes) \n",
      "23/09/15 01:42:53 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 12) (172.29.31.176, executor driver, partition 2, PROCESS_LOCAL, 8976 bytes) \n",
      "23/09/15 01:42:53 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 13) (172.29.31.176, executor driver, partition 3, PROCESS_LOCAL, 8276 bytes) \n",
      "23/09/15 01:42:53 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)\n",
      "23/09/15 01:42:53 INFO Executor: Running task 2.0 in stage 8.0 (TID 12)\n",
      "23/09/15 01:42:53 INFO Executor: Running task 3.0 in stage 8.0 (TID 13)\n",
      "23/09/15 01:42:53 INFO Executor: Running task 1.0 in stage 8.0 (TID 11)\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=1/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-92570, partition values: [2021,1]\n",
      "23/09/15 01:42:53 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.29.31.176:34579 in memory (size: 35.4 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=2/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-29961, partition values: [2020,2]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=6/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-70806, partition values: [2020,6]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=10/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-85021, partition values: [2020,10]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=3/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-91024, partition values: [2021,3]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=1/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-18944, partition values: [2020,1]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=5/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-69194, partition values: [2020,5]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=2/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-82866, partition values: [2021,2]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=12/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-90446, partition values: [2020,12]\n",
      "23/09/15 01:42:53 INFO Executor: Finished task 3.0 in stage 8.0 (TID 13). 2284 bytes result sent to driver\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=8/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-80567, partition values: [2020,8]\n",
      "23/09/15 01:42:53 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 13) in 681 ms on 172.29.31.176 (executor driver) (1/4)\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=4/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-63397, partition values: [2020,4]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=4/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-89024, partition values: [2021,4]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=9/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-79051, partition values: [2020,9]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=5/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-45684, partition values: [2021,5]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=11/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-85908, partition values: [2020,11]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=3/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-44720, partition values: [2020,3]\n",
      "23/09/15 01:42:53 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=7/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-76705, partition values: [2020,7]\n",
      "23/09/15 01:42:54 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 2284 bytes result sent to driver\n",
      "23/09/15 01:42:54 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 981 ms on 172.29.31.176 (executor driver) (2/4)\n",
      "23/09/15 01:42:54 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.29.31.176:34579 in memory (size: 22.2 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:42:54 INFO Executor: Finished task 1.0 in stage 8.0 (TID 11). 2241 bytes result sent to driver\n",
      "23/09/15 01:42:54 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 11) in 1013 ms on 172.29.31.176 (executor driver) (3/4)\n",
      "23/09/15 01:42:54 INFO Executor: Finished task 2.0 in stage 8.0 (TID 12). 2241 bytes result sent to driver\n",
      "23/09/15 01:42:54 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 12) in 1019 ms on 172.29.31.176 (executor driver) (4/4)\n",
      "23/09/15 01:42:54 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "23/09/15 01:42:54 INFO DAGScheduler: ShuffleMapStage 8 (showString at NativeMethodAccessorImpl.java:0) finished in 1.106 s\n",
      "23/09/15 01:42:54 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/09/15 01:42:54 INFO DAGScheduler: running: Set()\n",
      "23/09/15 01:42:54 INFO DAGScheduler: waiting: Set()\n",
      "23/09/15 01:42:54 INFO DAGScheduler: failed: Set()\n",
      "23/09/15 01:42:54 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1458521, minimum partition size: 1048576\n",
      "23/09/15 01:42:54 INFO CodeGenerator: Code generated in 16.161896 ms\n",
      "23/09/15 01:42:54 INFO CodeGenerator: Code generated in 20.29482 ms\n",
      "23/09/15 01:42:54 INFO CodeGenerator: Code generated in 33.090196 ms\n",
      "23/09/15 01:42:54 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/09/15 01:42:54 INFO DAGScheduler: Got job 8 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/15 01:42:54 INFO DAGScheduler: Final stage: ResultStage 10 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/09/15 01:42:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
      "23/09/15 01:42:54 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/15 01:42:54 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[39] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/15 01:42:54 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 54.0 KiB, free 3.4 GiB)\n",
      "23/09/15 01:42:54 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 21.0 KiB, free 3.4 GiB)\n",
      "23/09/15 01:42:54 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.29.31.176:34579 (size: 21.0 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:42:54 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/15 01:42:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[39] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/15 01:42:54 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "23/09/15 01:42:54 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 14) (172.29.31.176, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/09/15 01:42:54 INFO Executor: Running task 0.0 in stage 10.0 (TID 14)\n",
      "23/09/15 01:42:54 INFO ShuffleBlockFetcherIterator: Getting 4 (1400.2 KiB) non-empty blocks including 4 (1400.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/15 01:42:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "23/09/15 01:42:54 INFO CodeGenerator: Code generated in 15.667792 ms\n",
      "23/09/15 01:42:54 INFO CodeGenerator: Code generated in 33.966501 ms\n",
      "23/09/15 01:42:55 INFO CodeGenerator: Code generated in 20.837623 ms(0 + 1) / 1]\n",
      "23/09/15 01:42:55 INFO CodeGenerator: Code generated in 12.600875 ms\n",
      "23/09/15 01:42:55 INFO CodeGenerator: Code generated in 7.746845 ms\n",
      "23/09/15 01:42:55 INFO CodeGenerator: Code generated in 7.041442 ms\n",
      "23/09/15 01:42:55 INFO CodeGenerator: Code generated in 24.884847 ms\n",
      "23/09/15 01:42:55 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.29.31.176:34579 in memory (size: 8.4 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:42:55 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:42:56 INFO CodeGenerator: Code generated in 16.857 ms\n",
      "23/09/15 01:42:56 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------------------+------------------+-----------------------+----+\n",
      "|       pais|               data|media_movel_confirmados|media_movel_mortes|media_movel_recuperados| ano|\n",
      "+-----------+-------------------+-----------------------+------------------+-----------------------+----+\n",
      "|Afghanistan|2020-01-22 00:00:00|                      0|                 0|                      0|2020|\n",
      "|Afghanistan|2020-01-22 00:00:00|                      0|                 0|                      0|2020|\n",
      "|Afghanistan|2020-01-22 00:00:00|                      0|                 0|                      0|2020|\n",
      "|Afghanistan|2020-01-23 00:00:00|                      0|                 0|                      0|2020|\n",
      "|Afghanistan|2020-01-23 00:00:00|                      0|                 0|                      0|2020|\n",
      "|Afghanistan|2020-01-23 00:00:00|                      0|                 0|                      0|2020|\n",
      "|Afghanistan|2020-01-24 00:00:00|                      0|                 0|                      0|2020|\n",
      "|Afghanistan|2020-01-24 00:00:00|                      0|                 0|                      0|2020|\n",
      "|Afghanistan|2020-01-24 00:00:00|                      0|                 0|                      0|2020|\n",
      "|Afghanistan|2020-01-25 00:00:00|                      0|                 0|                      0|2020|\n",
      "+-----------+-------------------+-----------------------+------------------+-----------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/15 01:42:56 INFO Executor: Finished task 0.0 in stage 10.0 (TID 14). 5705 bytes result sent to driver\n",
      "23/09/15 01:42:56 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 14) in 2090 ms on 172.29.31.176 (executor driver) (1/1)\n",
      "23/09/15 01:42:56 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "23/09/15 01:42:56 INFO DAGScheduler: ResultStage 10 (showString at NativeMethodAccessorImpl.java:0) finished in 2.112 s\n",
      "23/09/15 01:42:56 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/15 01:42:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "23/09/15 01:42:56 INFO DAGScheduler: Job 8 finished: showString at NativeMethodAccessorImpl.java:0, took 2.133519 s\n",
      "23/09/15 01:42:56 INFO CodeGenerator: Code generated in 13.699281 ms            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/15 01:42:56 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.29.31.176:34579 in memory (size: 21.0 KiB, free: 3.4 GiB)\n"
     ]
    }
   ],
   "source": [
    "refined_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_df = refined_df.repartition(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/15 01:47:00 INFO DataSourceStrategy: Pruning directories with: \n",
      "23/09/15 01:47:00 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/09/15 01:47:00 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/09/15 01:47:01 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 202.0 KiB, free 3.4 GiB)\n",
      "23/09/15 01:47:01 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 3.4 GiB)\n",
      "23/09/15 01:47:01 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.29.31.176:34579 (size: 35.2 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:47:01 INFO SparkContext: Created broadcast 13 from save at NativeMethodAccessorImpl.java:0\n",
      "23/09/15 01:47:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 18124764 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/09/15 01:47:01 INFO DAGScheduler: Registering RDD 43 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "23/09/15 01:47:01 INFO DAGScheduler: Got map stage job 9 (save at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "23/09/15 01:47:01 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (save at NativeMethodAccessorImpl.java:0)\n",
      "23/09/15 01:47:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/09/15 01:47:01 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/15 01:47:01 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[43] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/15 01:47:01 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 19.9 KiB, free 3.4 GiB)\n",
      "23/09/15 01:47:01 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 3.4 GiB)\n",
      "23/09/15 01:47:01 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.29.31.176:34579 (size: 8.4 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:47:01 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/15 01:47:01 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[43] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "23/09/15 01:47:01 INFO TaskSchedulerImpl: Adding task set 11.0 with 4 tasks resource profile 0\n",
      "23/09/15 01:47:01 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 15) (172.29.31.176, executor driver, partition 0, PROCESS_LOCAL, 8983 bytes) \n",
      "23/09/15 01:47:01 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 16) (172.29.31.176, executor driver, partition 1, PROCESS_LOCAL, 8977 bytes) \n",
      "23/09/15 01:47:01 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 17) (172.29.31.176, executor driver, partition 2, PROCESS_LOCAL, 8976 bytes) \n",
      "23/09/15 01:47:01 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 18) (172.29.31.176, executor driver, partition 3, PROCESS_LOCAL, 8276 bytes) \n",
      "23/09/15 01:47:01 INFO Executor: Running task 0.0 in stage 11.0 (TID 15)\n",
      "23/09/15 01:47:01 INFO Executor: Running task 2.0 in stage 11.0 (TID 17)\n",
      "23/09/15 01:47:01 INFO Executor: Running task 1.0 in stage 11.0 (TID 16)\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=1/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-92570, partition values: [2021,1]\n",
      "23/09/15 01:47:01 INFO Executor: Running task 3.0 in stage 11.0 (TID 18)\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=6/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-70806, partition values: [2020,6]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=10/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-85021, partition values: [2020,10]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=2/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-29961, partition values: [2020,2]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=2/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-82866, partition values: [2021,2]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=1/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-18944, partition values: [2020,1]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=3/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-91024, partition values: [2021,3]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=5/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-69194, partition values: [2020,5]\n",
      "23/09/15 01:47:01 INFO Executor: Finished task 3.0 in stage 11.0 (TID 18). 2284 bytes result sent to driver\n",
      "23/09/15 01:47:01 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 18) in 302 ms on 172.29.31.176 (executor driver) (1/4)\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=8/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-80567, partition values: [2020,8]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=4/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-63397, partition values: [2020,4]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=9/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-79051, partition values: [2020,9]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=12/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-90446, partition values: [2020,12]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=7/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-76705, partition values: [2020,7]\n",
      "23/09/15 01:47:01 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.29.31.176:34579 in memory (size: 35.2 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=5/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-45684, partition values: [2021,5]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2021/mes=4/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-89024, partition values: [2021,4]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=3/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-44720, partition values: [2020,3]\n",
      "23/09/15 01:47:01 INFO FileScanRDD: Reading File path: file:///home/sold/repositories/desafio-data-engineer/datalake/trusted/ano=2020/mes=11/part-00000-d144768f-a7fa-4c7f-bbfd-ef141dd486d9.c000.snappy.parquet, range: 0-85908, partition values: [2020,11]\n",
      "23/09/15 01:47:01 INFO Executor: Finished task 0.0 in stage 11.0 (TID 15). 2284 bytes result sent to driver\n",
      "23/09/15 01:47:01 INFO Executor: Finished task 1.0 in stage 11.0 (TID 16). 2241 bytes result sent to driver\n",
      "23/09/15 01:47:01 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 16) in 765 ms on 172.29.31.176 (executor driver) (2/4)\n",
      "23/09/15 01:47:01 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 15) in 767 ms on 172.29.31.176 (executor driver) (3/4)\n",
      "23/09/15 01:47:01 INFO Executor: Finished task 2.0 in stage 11.0 (TID 17). 2241 bytes result sent to driver\n",
      "23/09/15 01:47:01 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 17) in 788 ms on 172.29.31.176 (executor driver) (4/4)\n",
      "23/09/15 01:47:01 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "23/09/15 01:47:01 INFO DAGScheduler: ShuffleMapStage 11 (save at NativeMethodAccessorImpl.java:0) finished in 0.817 s\n",
      "23/09/15 01:47:01 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/09/15 01:47:01 INFO DAGScheduler: running: Set()\n",
      "23/09/15 01:47:01 INFO DAGScheduler: waiting: Set()\n",
      "23/09/15 01:47:01 INFO DAGScheduler: failed: Set()\n",
      "23/09/15 01:47:01 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1458521, minimum partition size: 1048576\n",
      "23/09/15 01:47:01 INFO CodeGenerator: Code generated in 14.380633 ms\n",
      "23/09/15 01:47:02 INFO DAGScheduler: Registering RDD 52 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "23/09/15 01:47:02 INFO DAGScheduler: Got map stage job 10 (save at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "23/09/15 01:47:02 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (save at NativeMethodAccessorImpl.java:0)\n",
      "23/09/15 01:47:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "23/09/15 01:47:02 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/15 01:47:02 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[52] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/15 01:47:02 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 52.8 KiB, free 3.4 GiB)\n",
      "23/09/15 01:47:02 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 3.4 GiB)\n",
      "23/09/15 01:47:02 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.29.31.176:34579 (size: 20.5 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:47:02 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/15 01:47:02 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[52] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "23/09/15 01:47:02 INFO TaskSchedulerImpl: Adding task set 13.0 with 4 tasks resource profile 0\n",
      "23/09/15 01:47:02 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 19) (172.29.31.176, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
      "23/09/15 01:47:02 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 20) (172.29.31.176, executor driver, partition 1, NODE_LOCAL, 7352 bytes) \n",
      "23/09/15 01:47:02 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 21) (172.29.31.176, executor driver, partition 2, NODE_LOCAL, 7352 bytes) \n",
      "23/09/15 01:47:02 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 22) (172.29.31.176, executor driver, partition 3, NODE_LOCAL, 7352 bytes) \n",
      "23/09/15 01:47:02 INFO Executor: Running task 0.0 in stage 13.0 (TID 19)\n",
      "23/09/15 01:47:02 INFO Executor: Running task 3.0 in stage 13.0 (TID 22)\n",
      "23/09/15 01:47:02 INFO Executor: Running task 2.0 in stage 13.0 (TID 21)\n",
      "23/09/15 01:47:02 INFO Executor: Running task 1.0 in stage 13.0 (TID 20)\n",
      "23/09/15 01:47:02 INFO ShuffleBlockFetcherIterator: Getting 4 (1412.0 KiB) non-empty blocks including 4 (1412.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/15 01:47:02 INFO ShuffleBlockFetcherIterator: Getting 4 (1474.3 KiB) non-empty blocks including 4 (1474.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/15 01:47:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "23/09/15 01:47:02 INFO ShuffleBlockFetcherIterator: Getting 4 (1400.2 KiB) non-empty blocks including 4 (1400.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/15 01:47:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "23/09/15 01:47:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
      "23/09/15 01:47:02 INFO ShuffleBlockFetcherIterator: Getting 4 (1410.8 KiB) non-empty blocks including 4 (1410.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/15 01:47:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "23/09/15 01:47:02 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:02 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:02 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:03 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:03 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.29.31.176:34579 in memory (size: 8.4 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:47:03 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:03 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:03 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:04 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:05 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:05 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:05 INFO Executor: Finished task 0.0 in stage 13.0 (TID 19). 5773 bytes result sent to driver\n",
      "23/09/15 01:47:05 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 19) in 3518 ms on 172.29.31.176 (executor driver) (1/4)\n",
      "23/09/15 01:47:05 INFO Executor: Finished task 2.0 in stage 13.0 (TID 21). 5773 bytes result sent to driver\n",
      "23/09/15 01:47:05 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 21) in 3581 ms on 172.29.31.176 (executor driver) (2/4)\n",
      "23/09/15 01:47:05 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:05 INFO Executor: Finished task 1.0 in stage 13.0 (TID 20). 5773 bytes result sent to driver\n",
      "23/09/15 01:47:05 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 20) in 3627 ms on 172.29.31.176 (executor driver) (3/4)\n",
      "23/09/15 01:47:05 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter\n",
      "23/09/15 01:47:05 INFO Executor: Finished task 3.0 in stage 13.0 (TID 22). 5773 bytes result sent to driver\n",
      "23/09/15 01:47:05 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 22) in 3738 ms on 172.29.31.176 (executor driver) (4/4)\n",
      "23/09/15 01:47:05 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "23/09/15 01:47:05 INFO DAGScheduler: ShuffleMapStage 13 (save at NativeMethodAccessorImpl.java:0) finished in 3.759 s\n",
      "23/09/15 01:47:05 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/09/15 01:47:05 INFO DAGScheduler: running: Set()\n",
      "23/09/15 01:47:05 INFO DAGScheduler: waiting: Set()\n",
      "23/09/15 01:47:05 INFO DAGScheduler: failed: Set()\n",
      "23/09/15 01:47:05 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/09/15 01:47:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/09/15 01:47:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/09/15 01:47:05 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/09/15 01:47:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/09/15 01:47:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/09/15 01:47:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/09/15 01:47:06 INFO CodeGenerator: Code generated in 18.500615 ms\n",
      "23/09/15 01:47:06 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "23/09/15 01:47:06 INFO DAGScheduler: Got job 11 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/09/15 01:47:06 INFO DAGScheduler: Final stage: ResultStage 16 (save at NativeMethodAccessorImpl.java:0)\n",
      "23/09/15 01:47:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "23/09/15 01:47:06 INFO DAGScheduler: Missing parents: List()\n",
      "23/09/15 01:47:06 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[55] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/09/15 01:47:06 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 247.0 KiB, free 3.4 GiB)\n",
      "23/09/15 01:47:06 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 90.4 KiB, free 3.4 GiB)\n",
      "23/09/15 01:47:06 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.29.31.176:34579 (size: 90.4 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:47:06 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535\n",
      "23/09/15 01:47:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[55] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/09/15 01:47:06 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "23/09/15 01:47:06 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 23) (172.29.31.176, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "23/09/15 01:47:06 INFO Executor: Running task 0.0 in stage 16.0 (TID 23)\n",
      "23/09/15 01:47:06 INFO ShuffleBlockFetcherIterator: Getting 4 (1926.1 KiB) non-empty blocks including 4 (1926.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/09/15 01:47:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "23/09/15 01:47:06 INFO CodeGenerator: Code generated in 19.896209 ms\n",
      "23/09/15 01:47:06 INFO CodeGenerator: Code generated in 14.723832 ms\n",
      "23/09/15 01:47:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/09/15 01:47:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/09/15 01:47:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/09/15 01:47:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/09/15 01:47:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/09/15 01:47:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "23/09/15 01:47:06 INFO CodeGenerator: Code generated in 9.238057 ms\n",
      "23/09/15 01:47:07 INFO CodeGenerator: Code generated in 10.529518 ms(0 + 1) / 1]\n",
      "23/09/15 01:47:07 INFO CodeGenerator: Code generated in 54.399158 ms\n",
      "23/09/15 01:47:07 INFO CodecConfig: Compression: SNAPPY\n",
      "23/09/15 01:47:07 INFO CodecConfig: Compression: SNAPPY\n",
      "23/09/15 01:47:07 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "23/09/15 01:47:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"pais\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"data\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"media_movel_confirmados\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"media_movel_mortes\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"media_movel_recuperados\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary pais (STRING);\n",
      "  optional int96 data;\n",
      "  optional int64 media_movel_confirmados;\n",
      "  optional int64 media_movel_mortes;\n",
      "  optional int64 media_movel_recuperados;\n",
      "}\n",
      "\n",
      "       \n",
      "23/09/15 01:47:07 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "23/09/15 01:47:09 INFO CodecConfig: Compression: SNAPPY\n",
      "23/09/15 01:47:09 INFO CodecConfig: Compression: SNAPPY\n",
      "23/09/15 01:47:09 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
      "23/09/15 01:47:09 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"pais\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"data\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"media_movel_confirmados\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"media_movel_mortes\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"media_movel_recuperados\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary pais (STRING);\n",
      "  optional int96 data;\n",
      "  optional int64 media_movel_confirmados;\n",
      "  optional int64 media_movel_mortes;\n",
      "  optional int64 media_movel_recuperados;\n",
      "}\n",
      "\n",
      "       \n",
      "23/09/15 01:47:10 INFO FileOutputCommitter: Saved output of task 'attempt_202309150147068157827601077400302_0016_m_000000_23' to file:/home/sold/repositories/desafio-data-engineer/datalake/refined/_temporary/0/task_202309150147068157827601077400302_0016_m_000000\n",
      "23/09/15 01:47:10 INFO SparkHadoopMapRedUtil: attempt_202309150147068157827601077400302_0016_m_000000_23: Committed. Elapsed time: 1 ms.\n",
      "23/09/15 01:47:10 INFO Executor: Finished task 0.0 in stage 16.0 (TID 23). 8343 bytes result sent to driver\n",
      "23/09/15 01:47:10 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 23) in 4040 ms on 172.29.31.176 (executor driver) (1/1)\n",
      "23/09/15 01:47:10 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "23/09/15 01:47:10 INFO DAGScheduler: ResultStage 16 (save at NativeMethodAccessorImpl.java:0) finished in 4.095 s\n",
      "23/09/15 01:47:10 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/09/15 01:47:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "23/09/15 01:47:10 INFO DAGScheduler: Job 11 finished: save at NativeMethodAccessorImpl.java:0, took 4.100902 s\n",
      "23/09/15 01:47:10 INFO FileFormatWriter: Start to commit write Job f84fdd33-96f4-420d-99a2-4bbe90569995.\n",
      "23/09/15 01:47:10 INFO FileFormatWriter: Write Job f84fdd33-96f4-420d-99a2-4bbe90569995 committed. Elapsed time: 46 ms.\n",
      "23/09/15 01:47:10 INFO FileFormatWriter: Finished processing stats for write job f84fdd33-96f4-420d-99a2-4bbe90569995.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/15 01:47:27 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.29.31.176:34579 in memory (size: 20.5 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:47:27 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.29.31.176:34579 in memory (size: 90.4 KiB, free: 3.4 GiB)\n",
      "23/09/15 01:47:27 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.29.31.176:34579 in memory (size: 35.2 KiB, free: 3.4 GiB)\n"
     ]
    }
   ],
   "source": [
    "refined_df.write.mode('overwrite').format('parquet').partitionBy(partition_fields).option(\"parquet.compress\", \"snappy\").save(write_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
