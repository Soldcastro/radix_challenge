{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import types as T\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/14 18:06:14 WARN Utils: Your hostname, DESKTOP-TME356J resolves to a loopback address: 127.0.1.1; using 172.29.31.176 instead (on interface eth0)\n",
      "23/09/14 18:06:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/14 18:06:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"airflow_app\") \\\n",
    "    .config('spark.executor.memory', '6g') \\\n",
    "    .config('spark.driver.memory', '6g') \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1048MB\") \\\n",
    "    .config(\"spark.port.maxRetries\", \"100\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "#sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = 'datalake/raw/covid19'\n",
    "write_path = 'datalake/trusted'\n",
    "partition_fields = ['ano', 'mes']\n",
    "partition = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .options(header='true', delimiter=',') \\\n",
    "        .load(read_path)\n",
    "except Exception:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_unpivot = [f\"`{c}`, \\'{c}\\'\" for c in df.columns if c not in ['Province/State','Country/Region','Lat','Long']]\n",
    "stack_string = \", \".join(cols_to_unpivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpivot = df.select(\n",
    "    'Province/State','Country/Region','Lat','Long',\n",
    "    F.expr(f\"stack({len(cols_to_unpivot)}, {stack_string}) as (Cases, Date)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpivot = df_unpivot.withColumn('Date', F.to_timestamp('Date','M/d/yy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/14 18:07:29 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------+---------+-----+-------------------+\n",
      "|Province/State|Country/Region|     Lat|     Long|Cases|               Date|\n",
      "+--------------+--------------+--------+---------+-----+-------------------+\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-22 00:00:00|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-23 00:00:00|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-24 00:00:00|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-25 00:00:00|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-26 00:00:00|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-27 00:00:00|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-28 00:00:00|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-29 00:00:00|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-30 00:00:00|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-31 00:00:00|\n",
      "+--------------+--------------+--------+---------+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_unpivot.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpivot.filter(\"Date is null\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpivot.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Long: string (nullable = true)\n",
      " |-- Cases: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Source_file: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_unpivot = df_unpivot.withColumn('Source_file', F.input_file_name())\n",
    "df_unpivot.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Long: string (nullable = true)\n",
      " |-- Cases: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_unpivot = df_unpivot.withColumn('Source', F.when(F.col('Source_file').contains('confirmed') , 'confirmed')\n",
    "                                .when(F.col('Source_file').contains('deaths') , 'deaths')\n",
    "                                .when(F.col('Source_file').contains('recovered') , 'recovered')\n",
    "                                ).drop('Source_file')\n",
    "df_unpivot.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------+---------+-----+-------------------+---------+\n",
      "|Province/State|Country/Region|     Lat|     Long|Cases|               Date|   Source|\n",
      "+--------------+--------------+--------+---------+-----+-------------------+---------+\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-22 00:00:00|confirmed|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-23 00:00:00|confirmed|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-24 00:00:00|confirmed|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-25 00:00:00|confirmed|\n",
      "|          null|   Afghanistan|33.93911|67.709953|    0|2020-01-26 00:00:00|confirmed|\n",
      "+--------------+--------------+--------+---------+-----+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_unpivot.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+---+----+-----+----+------+\n",
      "|Province/State|Country/Region|Lat|Long|Cases|Date|Source|\n",
      "+--------------+--------------+---+----+-----+----+------+\n",
      "+--------------+--------------+---+----+-----+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_unpivot.filter(F.col(\"Cases\").cast(\"int\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('Country/Region','Source').orderBy('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpivot = df_unpivot.withColumn('Cases_increase', F.col(\"Cases\") - F.lag(\"Cases\", 1, 0).over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpivot = df_unpivot.withColumnRenamed('Province/State','Province_State').withColumnRenamed('Country/Region', 'Country_Region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Province_State: string (nullable = true)\n",
      " |-- Country_Region: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Long: string (nullable = true)\n",
      " |-- Cases: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Source: string (nullable = true)\n",
      " |-- Cases_increase: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_unpivot.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpivot.createOrReplaceTempView('raw_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from raw_table limit 100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trusted_sql = (\"select Country_Region as pais, \\\n",
    "               Province_State as estado, \\\n",
    "               Lat as latitude, \\\n",
    "               Long as longitude, \\\n",
    "               Date as data, \\\n",
    "               case when Source == 'confirmed' then Cases_increase end as quantidade_confirmados, \\\n",
    "               case when Source == 'deaths' then Cases_increase end as quantidade_mortes, \\\n",
    "               case when Source == 'recovered' then Cases end as quantidade_recuperados \\\n",
    "               from raw_table \\\n",
    "               \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_trusted = spark.sql(trusted_sql)\n",
    "except Exception:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted = df_trusted.withColumn(\"ano\", F.year(\"data\")).withColumn(\"mes\", F.month(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted = df_trusted.withColumn('latitude', F.col('latitude').cast(\"double\"))\n",
    "df_trusted = df_trusted.withColumn('longitude', F.col('longitude').cast(\"double\"))\n",
    "df_trusted = df_trusted.withColumn('quantidade_confirmados', F.col('quantidade_confirmados').cast(\"long\"))\n",
    "df_trusted = df_trusted.withColumn('quantidade_mortes', F.col('quantidade_mortes').cast(\"long\"))\n",
    "df_trusted = df_trusted.withColumn('quantidade_recuperados', F.col('quantidade_recuperados').cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pais: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- quantidade_confirmados: long (nullable = true)\n",
      " |-- quantidade_mortes: long (nullable = true)\n",
      " |-- quantidade_recuperados: long (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trusted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted = df_trusted.repartition(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted.write.mode('overwrite').format('parquet').partitionBy(partition_fields).option(\"parquet.compress\", \"snappy\").save(write_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
